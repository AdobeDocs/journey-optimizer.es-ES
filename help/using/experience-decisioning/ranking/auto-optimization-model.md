---
product: experience platform
solution: Experience Platform
title: Modelos de optimizaci√≥n autom√°tica
description: M√°s informaci√≥n sobre los modelos de optimizaci√≥n autom√°tica
feature: Ranking, Decision Management
role: User
level: Experienced
exl-id: 8a8b66cb-dd96-4373-bbe0-a67e0dc0b2c0
version: Journey Orchestration
source-git-commit: 0b94bfeaf694e8eaf0dd85e3c67ee97bd9b56294
workflow-type: tm+mt
source-wordcount: '1500'
ht-degree: 0%

---

# Modelos de optimizaci√≥n autom√°tica {#auto-optimization-model}

Un modelo de optimizaci√≥n autom√°tica tiene como objetivo ofrecer ofertas que maximicen el rendimiento (KPI) establecido por los clientes empresariales. Estos KPI pueden adoptar la forma de tasas de conversi√≥n, ingresos, etc. En este punto, la optimizaci√≥n autom√°tica se centra en optimizar los clics de oferta con la conversi√≥n de ofertas como objetivo. La optimizaci√≥n autom√°tica no est√° personalizada y se optimiza en funci√≥n del rendimiento &quot;global&quot; de las ofertas.

## Requisitos del conjunto de datos

Para entrenar un modelo de optimizaci√≥n autom√°tica, el conjunto de datos debe cumplir los siguientes requisitos m√≠nimos:

* Al menos 2 ofertas del conjunto de datos deben tener al menos 100 eventos de visualizaci√≥n y 5 eventos de clic en los √∫ltimos 14 d√≠as.
* El modelo tratar√° las ofertas con menos de 100 visualizaciones o eventos de 5 clics en los √∫ltimos 14 d√≠as como nuevas ofertas y solo son elegibles para ser servidas por el bandido de exploraci√≥n.
* El modelo tratar√° las ofertas con m√°s de 100 visualizaciones y eventos de 5 clics en los √∫ltimos 14 d√≠as como ofertas existentes y pueden ser servidas por bandidos de exploraci√≥n y explotaci√≥n.

Hasta la primera vez que se entrene un modelo de optimizaci√≥n autom√°tica, las ofertas dentro de una estrategia de selecci√≥n que utilice un modelo de optimizaci√≥n autom√°tica se servir√°n al azar.

## Limitaciones {#limitations}

El uso de modelos de optimizaci√≥n autom√°tica para la administraci√≥n de decisiones est√° sujeto a las siguientes limitaciones:

<!--* Auto-optimization models do not work with the Batch Decisioning API.-->
* Los comentarios necesarios para crear un modelo deben enviarse como un evento de experiencia. No se debe enviar autom√°ticamente en [!DNL Journey Optimizer] canales.

## Terminolog√≠a {#terminology}

Los siguientes t√©rminos pueden resultar √∫tiles al tratar el tema de la optimizaci√≥n autom√°tica:

* **Multi-armed bandit**: Un enfoque de [multi-armed bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit){target="_blank"} en la optimizaci√≥n equilibra el aprendizaje exploratorio y la explotaci√≥n de ese aprendizaje.

* **Muestreo Thomson**: El muestreo Thompson es un algoritmo para problemas de decisiones en l√≠nea en el que las acciones se toman secuencialmente de una manera que debe equilibrar entre la explotaci√≥n de lo que se sabe que maximiza el rendimiento inmediato y la inversi√≥n para acumular informaci√≥n nueva que pueda mejorar el rendimiento futuro. [M√°s informaci√≥n](#thompson-sampling)

* [**Distribuci√≥n de Beta**](https://en.wikipedia.org/wiki/Beta_distribution){target="_blank"}: conjunto de [distribuciones de probabilidad](https://en.wikipedia.org/wiki/Probability_distribution){target="_blank"} continuas definidas en el intervalo [0, 1] [parametrizadas](https://en.wikipedia.org/wiki/Statistical_parameter){target="_blank"} por dos [par√°metros de forma](https://en.wikipedia.org/wiki/Shape_parameter){target="_blank"} positivos.

## Muestreo Thompson {#thompson-sampling}

El algoritmo subyacente a la optimizaci√≥n autom√°tica es **Muestreo Thompson**. En esta secci√≥n, analizamos la intuici√≥n detr√°s del muestreo Thompson.

[Muestreo Thompson](https://en.wikipedia.org/wiki/Thompson_sampling){target="_blank"}, o bandidos bayesianos, es un enfoque bayesiano del problema de los bandidos multiarmados.  La idea b√°sica es tratar la recompensa promedio ùõç de cada oferta como una **variable aleatoria** y usar los datos que hemos recopilado hasta el momento para actualizar nuestra &quot;creencia&quot; sobre la recompensa promedio. Esta &quot;creencia&quot; est√° representada matem√°ticamente por una **distribuci√≥n posterior de probabilidad** - esencialmente un rango de valores para la recompensa promedio, junto con la plausibilidad (o probabilidad) de que la recompensa tenga ese valor para cada oferta.‚ÄØLuego, por cada decisi√≥n, **tomaremos una muestra de un punto de cada una de estas distribuciones de recompensa posterior** y seleccionaremos la oferta cuya recompensa incluida ten√≠a el valor m√°s alto.

Este proceso se ilustra en la figura siguiente, donde tenemos 3 ofertas diferentes. Inicialmente no tenemos evidencia de los datos y asumimos que todas las ofertas tienen una distribuci√≥n de recompensa posterior uniforme. Tomamos una muestra de la distribuci√≥n posterior de recompensas de cada oferta. La muestra seleccionada en la distribuci√≥n de la oferta 2 tiene el valor m√°s alto. Este es un ejemplo de **exploraci√≥n**. Despu√©s de mostrar la Oferta 2, recopilamos cualquier recompensa potencial (por ejemplo, conversi√≥n/no conversi√≥n) y actualizamos la distribuci√≥n posterior de la Oferta 2 usando el Teorema de Bayes como se explica a continuaci√≥n.  Continuamos este proceso y actualizamos las distribuciones posteriores cada vez que se muestra una oferta y se obtiene la recompensa. En la segunda cifra, se selecciona la Oferta 3: a pesar de que la Oferta 1 tiene la recompensa promedio m√°s alta (su distribuci√≥n de recompensa posterior est√° m√°s a la derecha), el proceso de muestreo de cada distribuci√≥n nos ha llevado a elegir una Oferta 3 aparentemente sub√≥ptima. Al hacerlo, nos damos la oportunidad de aprender m√°s acerca de la verdadera distribuci√≥n de recompensas de Offer 3.

A medida que se recogen m√°s muestras, la confianza aumenta y se obtiene una estimaci√≥n m√°s precisa de la posible recompensa (correspondiente a distribuciones de recompensa m√°s estrechas).‚ÄØEste proceso de actualizar nuestras creencias a medida que se dispone de m√°s evidencia se conoce como **Inferencia bayesiana**.

Finalmente, si una oferta (por ejemplo, la oferta 1) es un claro ganador, su distribuci√≥n de recompensa posterior se separar√° de las dem√°s. En este punto, para cada decisi√≥n, es probable que la recompensa muestreada de la Oferta 1 sea la m√°s alta y la elegiremos con una mayor probabilidad. Esta es **explotaci√≥n** - creemos firmemente que la Oferta 1 es la mejor, y por lo tanto se elige para maximizar las recompensas.

![](../assets/ai-ranking-thompson-sampling.png)

**Figura 1**: *Por cada decisi√≥n, tomamos una muestra de un punto de las distribuciones de recompensa posteriores. Se elige la oferta con el valor de muestra m√°s alto (tasa de conversi√≥n). En la fase inicial, todas las ofertas tienen una distribuci√≥n uniforme, ya que no tenemos ninguna evidencia sobre las tasas de conversi√≥n de las ofertas a partir de los datos. A medida que recogemos m√°s muestras, las distribuciones posteriores se vuelven m√°s estrechas y precisas. En √∫ltima instancia, se elegir√° siempre la oferta con la tasa de conversi√≥n m√°s alta.*

+++**Detalles t√©cnicos**

Para calcular/actualizar distribuciones, usamos el **Teorema de Bayes**. Para cada oferta ***i***, queremos calcular su ***P(ùõçi | data)***, es decir, para cada oferta ***i***, la probabilidad de que haya un valor de recompensa **ùõçi**, dados los datos que hemos recopilado hasta ahora para esa oferta.

Del Teorema De Bayes:

***Posterior = Probabilidad * Anterior***

La **probabilidad anterior** es la suposici√≥n inicial acerca de la probabilidad de producir un resultado. La probabilidad, despu√©s de que se hayan recopilado algunas pruebas, se conoce como la **probabilidad posterior**.‚ÄØ

La optimizaci√≥n autom√°tica est√° dise√±ada para tener en cuenta las recompensas binarias (clic/sin clic). En este caso, la probabilidad representa el n√∫mero de √©xitos de N ensayos y est√° modelada por una **distribuci√≥n binomial**. Para algunas funciones de probabilidad, si se elige una determinada anterior, la posterior termina estando en la misma distribuci√≥n que la anterior. A este tipo de prior se le denomina **conjugado prior**. Este tipo de antecedente hace que el c√°lculo de la distribuci√≥n posterior sea muy sencillo. La **distribuci√≥n Beta** es un conjugado anterior a la probabilidad binomial (recompensas binarias), por lo que es una opci√≥n conveniente y sensata para las distribuciones de probabilidad anterior y posterior. La distribuci√≥n Beta toma dos par√°metros, ****** y ******.‚ÄØEstos par√°metros pueden considerarse como el recuento de √©xitos y errores y el valor medio dado por:

![](../assets/ai-ranking-beta-distribution.png)

La funci√≥n de probabilidad como explicamos arriba est√° modelada por una distribuci√≥n binomial, con s √©xitos (conversiones) y f errores (sin conversiones) y q es una [variable aleatoria](https://en.wikipedia.org/wiki/Random_variable){target="_blank"} con una [distribuci√≥n beta](https://en.wikipedia.org/wiki/Beta_distribution){target="_blank"}.

La distribuci√≥n anterior se modela mediante Beta y la posterior toma la siguiente forma:

![](../assets/ai-ranking-posterior-distribution.svg)

La parte posterior se calcula simplemente agregando el n√∫mero de aciertos y errores a los par√°metros existentes ******, ******.

Para la optimizaci√≥n autom√°tica, como se muestra en el ejemplo anterior, comenzamos con una distribuci√≥n anterior ***Beta(1, 1)*** (distribuci√≥n uniforme) para todas las ofertas y despu√©s de obtener los √©xitos y los errores de una oferta determinada, la posterior se convierte en una distribuci√≥n Beta con los par√°metros ***(s+, f+)*** para esa oferta.
+++

**Temas relacionados**:

Para profundizar en el muestreo Thompson, lea los siguientes art√≠culos de investigaci√≥n:

* [Evaluaci√≥n emp√≠rica del muestreo Thompson](https://proceedings.neurips.cc/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf){target="_blank"}
* [An√°lisis del muestreo Thompson para el problema Multi-armed Bandit](https://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf){target="_blank"}

## Problema de arranque en fr√≠o {#cold-start}

El problema de &quot;inicio en fr√≠o&quot; se produce cuando se agrega una nueva oferta a una campa√±a y no hay datos disponibles sobre la tasa de conversi√≥n de la nueva oferta. Durante este periodo, tenemos que idear una estrategia con respecto a la frecuencia con la que se elige esta nueva oferta para minimizar la ca√≠da de rendimiento, mientras recopilamos informaci√≥n sobre la tasa de conversi√≥n de esta nueva oferta. Hay m√∫ltiples soluciones disponibles para abordar este problema. La clave es encontrar un equilibrio entre la exploraci√≥n de esta nueva oferta mientras no sacrificamos mucho la explotaci√≥n. Actualmente utilizamos &quot;distribuci√≥n uniforme&quot; como aproximaci√≥n inicial sobre la tasa de conversi√≥n de la nueva oferta (distribuci√≥n anterior). B√°sicamente, damos a todos los valores de tasa de conversi√≥n la misma probabilidad de ocurrencia.

![](../assets/ai-ranking-cold-start-strategies.png)

**Figura 2**: *Considere una campa√±a con 3 ofertas. Mientras la campa√±a est√° activa, la oferta 4 se a√±ade a la campa√±a. Inicialmente no tenemos datos sobre la tasa de conversi√≥n de la oferta 4 y tenemos que hacer frente al problema de arranque en fr√≠o. Utilizamos la distribuci√≥n uniforme como nuestra estimaci√≥n inicial sobre la tasa de conversi√≥n de la oferta 4, mientras que recopilamos datos para esta nueva oferta. Como se explica en la secci√≥n [Muestreo Thompson](#thompson-sampling), para elegir qu√© oferta se mostrar√° a un usuario, tomamos muestras de los puntos de las distribuciones de recompensas posteriores de las ofertas y seleccionamos la oferta con el valor de muestra m√°s alto. En el ejemplo anterior, se elige la oferta 4 y m√°s adelante en funci√≥n de la recompensa obtenida, la distribuci√≥n posterior de esta oferta se actualiza tal como se explica en la secci√≥n [Muestreo Thompson](#thompson-sampling).*

## Medida de alza {#lift}

&quot;Alza&quot; es la m√©trica utilizada para medir el rendimiento de cualquier estrategia implementada en el servicio de clasificaci√≥n, en comparaci√≥n con la estrategia de l√≠nea de base (servir ofertas solo aleatoriamente).

Por ejemplo, si estamos interesados en medir el rendimiento de una estrategia de Muestreo Thompson (TS) utilizada en el servicio de clasificaci√≥n y el KPI es la tasa de conversi√≥n (CVR), el &quot;alza&quot; de la estrategia de TS respecto a la estrategia de l√≠nea de base se define como:

![](../assets/ai-ranking-lift.png)

>[!NOTE]
>
>Actualmente, el informe Medici√≥n de alza solo est√° disponible para el modelo de IA [Optimizaci√≥n personalizada](personalized-optimization-model.md). [M√°s informaci√≥n sobre los informes de decisiones](../../reports/campaign-global-report-cja-code.md#decisioning-reporting)
